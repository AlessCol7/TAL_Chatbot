{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1555aabe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d5d75e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "ddfc27c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain_ibm import ChatWatsonx\n",
    "\n",
    "# model = ChatWatsonx(\n",
    "#     model_id=\"ibm/granite-20b-code-instruct\",\n",
    "#     url=\"https://eu-de.ml.cloud.ibm.com\",\n",
    "#     project_id=\"aeabba3a-e8d1-4181-bcff-d0a1b639e896\"\n",
    "# )\n",
    "\n",
    "# from langchain_huggingface import ChatHuggingFace, HuggingFaceEndpoint\n",
    "\n",
    "# llm = HuggingFaceEndpoint(\n",
    "#     repo_id=\"deepseek-ai/DeepSeek-R1\"\n",
    "# )\n",
    "\n",
    "# model = ChatHuggingFace(llm=llm)\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model  = ChatOllama(model=\"qwen3:4b\", temperature=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc33d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "embeddings = OllamaEmbeddings(model=\"nomic-embed-text:v1.5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1cc400e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = embeddings.embed_query(\"How are you\")\n",
    "len(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e9962740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_pinecone import PineconeVectorStore\n",
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"], \n",
    "              environment=\"us-east-1\")\n",
    "              \n",
    "index_name = \"tal-chatbot\"\n",
    "index = pc.Index(index_name)\n",
    "\n",
    "vector_store = PineconeVectorStore(embedding=embeddings, index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb836c",
   "metadata": {},
   "source": [
    "Loading Documents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "07e0204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import JSONLoader\n",
    "file_path = \"/Users/sathvika/MCT/SEM-4/industry-project/TAL_Chatbot-1/converters_with_links_and_pricelist.json\"\n",
    "\n",
    "loader = JSONLoader(\n",
    "    file_path=file_path,\n",
    "    jq_schema='.[]',  # This will iterate over each item\n",
    "    text_content=False  # We'll use the entire object as content\n",
    ")\n",
    "\n",
    "documents = loader.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b434a5e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "65"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf77259e",
   "metadata": {},
   "source": [
    "Uploading Embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "47117c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14b8285",
   "metadata": {},
   "source": [
    "Consine Sim Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c843793a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "custom_prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"\"\"You are a technical assistant for lighting equipment. Always use this JSON context to answer.\n",
    "     For any query about components:\n",
    "        1. List ALL matching items from context\n",
    "        2. Include full technical details\n",
    "        3. For minimum and maximum questions, refer to the min and max fields respectively\n",
    "        4. Always respond in markdown, using clear lists and tables where appropriate. Do not output JSON unless specifically asked\"\"\"),\n",
    "        (\"human\", \"\"\"Context: {context}\n",
    "            \n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "ed63e4cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"], k=100)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    # messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    # response = model.invoke(messages)\n",
    "    # return {\"answer\": response.content}\n",
    "    messages = custom_prompt.invoke({\n",
    "        \"question\": state[\"question\"],\n",
    "        \"context\": docs_content\n",
    "    })\n",
    "    \n",
    "    response = model.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "20f34f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking what lamps can be used with the converter model 930606. First, I need to look up the details for that specific converter.\n",
      "\n",
      "Looking at the data provided, the converter 930606 is described as \"POWERLED CONVERTER REMOTE 180mA 9W IP20 CASAMBI\" with strain relief and indoor location. The DIMMABILITY is CASAMBI, and the CCR (AMPLITUDE) is YES. The lamps section for this model lists several options.\n",
      "\n",
      "The lamps listed include Beaufort, Thinksmall halosphere single, Thinksmall halosphere double, Single led XPE, Thinksmall/floorspot WC luxeon MX, *MIX 6 monocolor, Cedrus quantum, *MIX 6 halosphere, MIX 13 monocolor, MIX 13 halosphere, ORBITAL monocolor, ORBITAL halosphere, Beaufort², Haloled, B4, MIX 26 monocolor, and *BOA WC. \n",
      "\n",
      "But I need to check the min and max values for each. For example, Beaufort has min 1 and max 1, so that's a single lamp. Thinksmall halosphere single has min 1 to max 2. The *MIX 6 monocolor has min 1 to max 2, and so on. \n",
      "\n",
      "However, the user is asking for the lamps that can be used, not the range. So the answer should list all the lamp types mentioned in the lamps section for 930606. But I need to make sure that the ranges are correct. For example, if a lamp has min 1 and max 2, that means it can be used in quantities from 1 to 2. But the question is about what lamps can be used, not the quantity. So the answer should list each lamp type with their respective min and max if necessary.\n",
      "\n",
      "Wait, the question is \"what lamps can I use for 930606?\" So the answer should list all the lamp types that are compatible with this converter. The answer should include each lamp type with their min and max values as per the data provided. So the answer would be a list of each lamp type with their min and max, as per the data given.\n",
      "</think>\n",
      "\n",
      "For the converter **930606** (POWERLED CONVERTER REMOTE 180mA 9W IP20 CASAMBI), the compatible lamps are:\n",
      "\n",
      "1. **Beaufort** – Min 1, Max 1  \n",
      "2. **Thinksmall halosphere single** – Min 1, Max 2  \n",
      "3. **Thinksmall halosphere double** – Min 1, Max 1  \n",
      "4. **Single led XPE** – Min 5, Max 12  \n",
      "5. **Thinksmall/floorspot WC luxeon MX** – Min 2, Max 3  \n",
      "6. **MIX 6 monocolor** – Min 1, Max 2  \n",
      "7. **Cedrus quantum** – Min 2, Max 2  \n",
      "8. **MIX 6 halosphere** – Min 2, Max 2  \n",
      "9. **MIX 13 monocolor** – Min 1, Max 1  \n",
      "10. **MIX 13 halosphere** – Min 1, Max 1  \n",
      "11. **ORBITAL monocolor** – Min 1, Max 1  \n",
      "12. **ORBITAL halosphere** – Min 1, Max 1  \n",
      "13. **Beaufort²** – Min 1, Max 1  \n",
      "14. **Haloled** – Min 2, Max 3  \n",
      "15. **B4** – Min 2, Max 3  \n",
      "16. **MIX 26 monocolor** – Min 1, Max 1  \n",
      "17. **BOA WC** – Min 2, Max 4  \n",
      "\n",
      "These lamps are compatible with the converter's specifications (CASAMBI dimming, 9W output, IP20 rating). Ensure the quantities (min/max) align with your installation requirements.\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"what lamps can i use for 930606?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a513ca7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
